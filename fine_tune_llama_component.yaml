# PIPELINE DEFINITION
# Name: fine-tune-llama
# Inputs:
#    base_model: str
#    batch_size: int
#    data_path: str
#    hf_token: str
#    learning_rate: float
#    num_epochs: int
#    output_dir: str
#    project_id: str
# Outputs:
#    Output: str
components:
  comp-fine-tune-llama:
    executorLabel: exec-fine-tune-llama
    inputDefinitions:
      parameters:
        base_model:
          parameterType: STRING
        batch_size:
          parameterType: NUMBER_INTEGER
        data_path:
          parameterType: STRING
        hf_token:
          parameterType: STRING
        learning_rate:
          parameterType: NUMBER_DOUBLE
        num_epochs:
          parameterType: NUMBER_INTEGER
        output_dir:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-fine-tune-llama:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - fine_tune_llama
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'transformers>=4.30.0'\
          \ 'datasets>=2.12.0' 'accelerate>=0.20.0' 'huggingface_hub>=0.16.4' 'bitsandbytes>=0.41.1'\
          \ 'scipy>=1.10.0' 'peft>=0.4.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef fine_tune_llama(\n    project_id: str,\n    data_path: str,\n\
          \    base_model: str,\n    num_epochs: int,\n    batch_size: int,\n    learning_rate:\
          \ float,\n    output_dir: str,\n    hf_token: str\n) -> str:\n    \"\"\"\
          Fine-tune LLaMA model component.\"\"\"\n    import os\n    import torch\n\
          \    from transformers import (\n        AutoModelForSequenceClassification,\n\
          \        AutoTokenizer,\n        Trainer,\n        TrainingArguments,\n\
          \        BitsAndBytesConfig,\n        DataCollatorWithPadding\n    )\n \
          \   from peft import (\n        LoraConfig,\n        get_peft_model,\n \
          \       prepare_model_for_kbit_training\n    )\n    from datasets import\
          \ load_dataset\n    from huggingface_hub import login, HfFolder\n    import\
          \ numpy as np\n\n    try:\n        # Print system information\n        print(f\"\
          PyTorch version: {torch.__version__}\")\n        print(f\"CUDA available:\
          \ {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n\
          \            print(f\"CUDA device count: {torch.cuda.device_count()}\")\n\
          \            print(f\"Current CUDA device: {torch.cuda.current_device()}\"\
          )\n            print(f\"Current CUDA device name: {torch.cuda.get_device_name()}\"\
          )\n\n        # Set environment variables\n        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"\
          ] = \"max_split_size_mb:512\"\n        os.environ[\"TOKENIZERS_PARALLELISM\"\
          ] = \"false\"\n        HfFolder.save_token(hf_token)\n\n        print(\"\
          Authenticating with Hugging Face...\")\n        login(token=hf_token)\n\n\
          \        print(\"Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(\n\
          \            base_model,\n            token=hf_token,\n            trust_remote_code=True\n\
          \        )\n\n        # Configure tokenizer\n        print(\"Configuring\
          \ tokenizer...\")\n        if tokenizer.pad_token is None:\n           \
          \ print(\"Setting pad_token to eos_token\")\n            tokenizer.pad_token\
          \ = tokenizer.eos_token\n            tokenizer.pad_token_id = tokenizer.eos_token_id\n\
          \n        print(\"Configuring quantization...\")\n        bnb_config = BitsAndBytesConfig(\n\
          \            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\"\
          ,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=False\n\
          \        )\n\n        print(f\"Loading model: {base_model}\")\n        model\
          \ = AutoModelForSequenceClassification.from_pretrained(\n            base_model,\n\
          \            token=hf_token,\n            num_labels=2,  # Binary classification\n\
          \            quantization_config=bnb_config,\n            device_map=\"\
          auto\",\n            trust_remote_code=True,\n            torch_dtype=torch.float16\n\
          \        )\n\n        # Configure model for padding\n        model.config.pad_token_id\
          \ = tokenizer.pad_token_id\n        model.config.use_cache = False  # For\
          \ gradient checkpointing\n\n        # Configure LoRA\n        lora_config\
          \ = LoraConfig(\n            r=16,\n            lora_alpha=32,\n       \
          \     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n\
          \            lora_dropout=0.05,\n            bias=\"none\",\n          \
          \  task_type=\"SEQ_CLS\"\n        )\n\n        print(\"Preparing model for\
          \ training...\")\n        model = prepare_model_for_kbit_training(model)\n\
          \        model = get_peft_model(model, lora_config)\n\n        print(\"\
          Loading IMDB dataset...\")\n        dataset = load_dataset(\"imdb\")\n \
          \       print(\"Dataset structure:\", dataset)\n\n        def preprocess_function(examples):\n\
          \            # Tokenize the texts\n            result = tokenizer(\n   \
          \             examples[\"text\"],\n                padding=\"max_length\"\
          ,\n                max_length=512,\n                truncation=True\n  \
          \          )\n            result[\"labels\"] = examples[\"label\"]\n   \
          \         return result\n\n        print(\"Preprocessing dataset...\")\n\
          \        tokenized_dataset = dataset.map(\n            preprocess_function,\n\
          \            batched=True,\n            remove_columns=dataset[\"train\"\
          ].column_names,\n            desc=\"Running tokenizer on dataset\"\n   \
          \     )\n\n        # Create data collator\n        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\
          \n        # Define compute metrics function\n        def compute_metrics(eval_pred):\n\
          \            predictions, labels = eval_pred\n            predictions =\
          \ np.argmax(predictions, axis=1)\n            accuracy = np.mean(predictions\
          \ == labels)\n            return {\"accuracy\": accuracy}\n\n        training_args\
          \ = TrainingArguments(\n            output_dir=output_dir,\n           \
          \ num_train_epochs=num_epochs,\n            per_device_train_batch_size=batch_size,\n\
          \            per_device_eval_batch_size=batch_size,\n            learning_rate=learning_rate,\n\
          \            logging_dir=f\"{output_dir}/logs\",\n            logging_steps=10,\n\
          \            save_strategy=\"epoch\",\n            evaluation_strategy=\"\
          epoch\",\n            save_total_limit=2,\n            load_best_model_at_end=True,\n\
          \            gradient_checkpointing=True,\n            fp16=True,\n    \
          \        optim=\"paged_adamw_32bit\",\n            gradient_accumulation_steps=4,\n\
          \            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\"\
          ,\n            weight_decay=0.01,\n            remove_unused_columns=False\n\
          \        )\n\n        trainer = Trainer(\n            model=model,\n   \
          \         args=training_args,\n            train_dataset=tokenized_dataset[\"\
          train\"],\n            eval_dataset=tokenized_dataset[\"test\"],\n     \
          \       tokenizer=tokenizer,\n            data_collator=data_collator,\n\
          \            compute_metrics=compute_metrics\n        )\n\n        print(\"\
          Starting training...\")\n        trainer.train()\n\n        print(f\"Saving\
          \ model to: {output_dir}\")\n        trainer.save_model(output_dir)\n  \
          \      model.save_pretrained(f\"{output_dir}/adapter\")\n        tokenizer.save_pretrained(output_dir)\n\
          \n        return output_dir\n\n    except Exception as e:\n        print(f\"\
          Error during model loading/training: {str(e)}\")\n        print(f\"Error\
          \ type: {type(e)}\")\n        import traceback\n        print(\"Full traceback:\"\
          )\n        print(traceback.format_exc())\n        raise e\n\n"
        image: gcr.io/deeplearning-platform-release/pytorch-gpu.1-13
pipelineInfo:
  name: fine-tune-llama
root:
  dag:
    outputs:
      parameters:
        Output:
          valueFromParameter:
            outputParameterKey: Output
            producerSubtask: fine-tune-llama
    tasks:
      fine-tune-llama:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-fine-tune-llama
        inputs:
          parameters:
            base_model:
              componentInputParameter: base_model
            batch_size:
              componentInputParameter: batch_size
            data_path:
              componentInputParameter: data_path
            hf_token:
              componentInputParameter: hf_token
            learning_rate:
              componentInputParameter: learning_rate
            num_epochs:
              componentInputParameter: num_epochs
            output_dir:
              componentInputParameter: output_dir
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: fine-tune-llama
  inputDefinitions:
    parameters:
      base_model:
        parameterType: STRING
      batch_size:
        parameterType: NUMBER_INTEGER
      data_path:
        parameterType: STRING
      hf_token:
        parameterType: STRING
      learning_rate:
        parameterType: NUMBER_DOUBLE
      num_epochs:
        parameterType: NUMBER_INTEGER
      output_dir:
        parameterType: STRING
      project_id:
        parameterType: STRING
  outputDefinitions:
    parameters:
      Output:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.1
